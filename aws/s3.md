---
title: S3
date: "2020-09-01T03:47:00.000Z"
description: "Simple Secure Storage"
---

# S3 - Simple Secure Storage

- Store Objects in **Buckets**
- Buckets should have global unique name
- Objects are files
    - key - path to the file
        - prefix 
        - object name
        - s3://bucket-name/my-folder/another_folder/myfile.txt
            - prefix - my-folder/another_folder/
            - objectName - myfile.txt
- max object size is 5 TB
- while uploading 5 GB at a time
    - if more have to go with muliti part upload
- versioning is supported

## Versioning

- has to be enabled in bucket level
- if uploaded same file to same folder different version will be stored
    - protection agains accidental deletion
    - easy role back
- when u delete a file with version enabled
    - s3 just add a delete marker to make it seem like it is deleted
    - so restoring is possible
    - if u delete a specific version it will be deleted permemenantly
- if u delete the delete marker it is restoring the file
- if u **suspend** bucket versioning after it is disabled
    - it will not delete version already created
    - newer versions will be created with null as the version id


## Encryption


- object level
- can provide a default encryption mechanisam in buckect
    - can overide the default mechanism during upload
- 4 methods for encryption
    - sse-s3 - keys handled by s3
        - server side encryption
        - AES -256
        - must set header **"x-amz-server-side-encryption":"AES256"**
    - sse-kms - using aws key manangment service
        - has user control and audit trial
        - must set header **"x-amz-server-side-encryption":"aws:kms"**
    - sse-c 
        - must be https
        - encryption key should be provided in HTTP header, but will be discarded
        - while retriving object need to specify the keys also
    - client side encryption
        - customer mananges the keys and encryption 
        - can use amazon s3 encryption client

> Block Public access can be done on bucket level- and there is an option account level to block public access for all the buckets

## Security 

- User Based 
    - IAM policies
- Resource Based
    - Bucket Policies
        - cross accoount access
        - json based policies
             ```json 
                {
                    "version":"2021-10-17",
                    "statement":[
                        {
                            "Sid":"PublicRead",
                            "Effect":"Allow",
                            "Principle":"*",
                            "Action":[
                                "s3:GetObject"
                            ],
                            "Resource":[
                                "arn:aws:s3:::examplebucket/*"
                            ],
                            "Condition":{
                                "StringNotEquals":{
                                    "s3:x-amz-server-side-encryption":"true"
                                }
                            }
                        }
                    ]
                }
        - uses
            - grand public access to bucket
            - Force objects to be encrypted at upload
            - grand cross account access
    - Object Access Control List
    - Bucket Access Control List


## Websites

- can host static websites
- cors config
    - ```json
        [
            {
                "AllowHeaders":[
                    "Authentication"
                ],
                "AllowMethods":[
                    "GET"
                ],
                "AllowedOrigins:[
                    "<url of first bucket with http://..without slash at the end>"
                ],
                "ExploseHeaders":[],
                "MaxAgeSeconds":3000
            }
        ]

## MFA Delete of s3 objects
- has to be enabled by aws cli of root user

## S3 Access Logs
-  logging all opertions on a s3 bucket to another s3 bucekt
- can use **Athena** to analise this logs
- **dont give same bucket for logging which will create an infinite logging**

## s3 Replication
- must enable versioning in source and destination
- Cross region replication (CRR)
- Same region replication (SRR)
- Buckets can be in different accounts
- async
- only new objects are replicated
- for delete operations can be replicated with optional setting
- cannot chain replications
- can be done for subset of objects

## pre-signed URL
- can only be generated using sdk or cli
- users given a pre-signed url will inherit the permissions of the person who generates the URL


## Storage Classes
- Amazon s3 Standard - General Purpose
    - Very high durablity
    - very high availabilty
    - sustain 2 concurrent facility failures
- Amazon s3 Standard - Infrequent Access IA
    - data that is less frequently accessed
    - less cost
    - high durablity
    - sustain 2 concurrent facility failures
    - minimum storage duration 30 days
- Amazon s3 One-zone Infrequent Access
    - same a IA but one AZ
    - minimum storage duration 30 days
    - lower cost when compared to IA (20%)'
- Amazon s3 Intelligent Tiering
    - same low latency and high thoughput as of s3 standard
    - smal montly monitoring and auto-tiering fee
    - automatically moves objects between access tiers based on access pattern
- Amazon Glazier
    - low cost storage
    - archiving and backups
    - cost is very low
    - item can be of size 40 tb
    - 3 retrival options
        - Expedited ( 1 to 5 mins)
        - Standard ( 3 to 5 hours)
        - Bulk ( 5 to 12 hourds)
        - Minimum storage duration of 90 days
- Amazon Glazier Deep Archive
    - super long term storage
    - cheaper than glazier
    - retrival options
        - Standard ( 12 hours )
        - Bulk ( 48 hours )
        - Minimumn storage duration 180 days

## Lifecycle Rules
- rule actions
    - current versions
    - pervious versions
    - expire current verions
    - permenantly delete previous versions
    - delete expired delete markers or incomplete mulitipart uploads
- transition actions
    - when objects are transitioned to another storage class
        - move objects to Standard IA class after creation
        - Move to glacier for archiving after 6 months
- Expiration actions
    - configure to delete after some time
        - access log deletion
    - can be used to delete older verison if versioning is enabled
    - cleanup of partial multip part uploads

## S3 Analytics
- setup s3 analytics to determin when to transition objects
- does not work for ONE zone IA or glazier
- report updated daily
- takes up to 24h to 48h to first start

## s3 Select and Glazier Select
- retrive less data using SQL by performing server side filtering
- can filter by rows and columns
- less network transer, less CPU cost client-side

## s3 Event Notifications
- usecase to generate thumnales for uploaded images
- can use
    - sns - simple notification service
    - sql - simple queue service
    - lambda functions
## Requester Pays Option
- if the requester has aws accont
    -networking cost can be billed to the requester


## Glazier Vault Lock
- Adopt a WORM ( Write Once Read Many ) modal
- lock the policy for future edits ( can no longer be changed )
- helpfull for compliance and data retention

# s3 Object lock
- Adopt WORM ( Write Once Read Many ) modal
- block object version deletion for a specified amount of time
- Object Retention
    - Retention period: fixed period
    - Legal hold: no expiry date
- Modes
    - Governance Mode
        - user cannot overwrite or delete an object version or alter its lock unless they have special permissions
    - Compliance Mode
        - protected object version cannot be overwritter or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened

 







